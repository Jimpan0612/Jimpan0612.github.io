[
  {
    "objectID": "Poster_Generation_with_AI.html",
    "href": "Poster_Generation_with_AI.html",
    "title": "6. Poster Generation with AI",
    "section": "",
    "text": "You have come to the wasteland of knowledge, which will be updated soon.\n\n\n\n Back to top",
    "crumbs": [
      "AI Art & Animation",
      "6. Poster Generation with AI"
    ]
  },
  {
    "objectID": "tex2img.html",
    "href": "tex2img.html",
    "title": "1. Text-to-Image with Draw Things",
    "section": "",
    "text": "Draw Things is a locally run AI image generation app designed for macOS, making it ideal for users who want to create AI-generated artwork on their Mac. When doing lightweight AI drawing, using Draw Things is simpler and more convenient than using ComfyUI.\nDownload Draw Things\nThis section demonstrates the Text-to-Image generation feature of Draw Things, where images are generated by providing positive and negative keywords with respective weights. The app allows users to adjust these parameters to influence the final output’s appearance, enabling control over attributes such as lighting, pose, style, and more.\nA key factor in the results is the checkpoint model used. Different checkpoint models can drastically affect the style of the generated image, whether it’s more realistic, anime-inspired, or stylized in a particular way. For example, using a more realistic checkpoint model can produce images with detailed skin textures and lifelike lighting, while an anime-style checkpoint model might create more vibrant, colorful, and exaggerated images.\nAdditionally, by incorporating Lora (Low-Rank Adaptation) models, you can fine-tune specific attributes in the images, such as facial expressions or clothing details.\n\n\n\n\n\nLayout Sample",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "tex2img.html#introduction",
    "href": "tex2img.html#introduction",
    "title": "1. Text-to-Image with Draw Things",
    "section": "",
    "text": "Draw Things is a locally run AI image generation app designed for macOS, making it ideal for users who want to create AI-generated artwork on their Mac. When doing lightweight AI drawing, using Draw Things is simpler and more convenient than using ComfyUI.\nDownload Draw Things\nThis section demonstrates the Text-to-Image generation feature of Draw Things, where images are generated by providing positive and negative keywords with respective weights. The app allows users to adjust these parameters to influence the final output’s appearance, enabling control over attributes such as lighting, pose, style, and more.\nA key factor in the results is the checkpoint model used. Different checkpoint models can drastically affect the style of the generated image, whether it’s more realistic, anime-inspired, or stylized in a particular way. For example, using a more realistic checkpoint model can produce images with detailed skin textures and lifelike lighting, while an anime-style checkpoint model might create more vibrant, colorful, and exaggerated images.\nAdditionally, by incorporating Lora (Low-Rank Adaptation) models, you can fine-tune specific attributes in the images, such as facial expressions or clothing details.\n\n\n\n\n\nLayout Sample",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "tex2img.html#positive-keywords",
    "href": "tex2img.html#positive-keywords",
    "title": "1. Text-to-Image with Draw Things",
    "section": "2 Positive Keywords",
    "text": "2 Positive Keywords\nIn this example, the following positive keywords were used to generate the images:\n(detailed eyes:1.3), elf, Beautiful Lighting, (1girl:(Thick thighs:0.8), blue eyes, blonde hair, absurdly long hair, ahoge, eyebrows visible through hair), (real skin), (outdoors:1.3), (alley:1.2), Pastel floral maxi dress, teardrop pearl earrings, beaded bracelet stack, lora:better_scar:1, (better_scar:1.2), scar on nose, (veins:1.21), (burn scar:1.21), (scar on nose:1.21), (scar on arm:1.21), (scar on Shoulder:1.21)",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "tex2img.html#negative-keywords",
    "href": "tex2img.html#negative-keywords",
    "title": "1. Text-to-Image with Draw Things",
    "section": "3 Negative Keywords",
    "text": "3 Negative Keywords\nNegative keywords help in avoiding unwanted details such as artifacts or incorrect body parts. In this case, the following negative keywords were applied:\nbad-hands-5, SkinPerfection_NegV15, multiple views, blurry, watermark, letterbox, text, see-through",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "tex2img.html#example-images",
    "href": "tex2img.html#example-images",
    "title": "1. Text-to-Image with Draw Things",
    "section": "4 Example Images",
    "text": "4 Example Images\nBelow are three images generated with the same prompt, showing how the model choice can drastically affect the style and result of the image:\n\n4.1 Example 1: Anime-style Model\n\n\n\nAnime Style\n\n\nThis image uses an anime-style model to generate a character with exaggerated proportions, pastel tones, and vibrant lighting effects.\n\n\n4.2 Example 2: Realistic Model\n\n\n\nRealistic Style\n\n\nA more realistic model produces natural skin tones and textures, with softer lighting and more life-like appearances of hair and clothing.\n\n\n4.3 Example 3: Storybook Model\n\n\n\nStorybook Style\n\n\nThis model aims for a whimsical, storybook-style appearance, with a softer, dream-like quality in the background and character rendering.",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "tex2img.html#influence-of-checkpoint-models",
    "href": "tex2img.html#influence-of-checkpoint-models",
    "title": "1. Text-to-Image with Draw Things",
    "section": "5 Influence of Checkpoint Models",
    "text": "5 Influence of Checkpoint Models\nThe reason for the vastly different styles among the generated images is the choice of checkpoint models. Each model is pre-trained on different datasets, which leads to its unique understanding of the input keywords. Additionally, Lora is used in this workflow to fine-tune certain attributes like scars and skin textures, further influencing the outcome.",
    "crumbs": [
      "AI Art & Animation",
      "1. Text-to-Image with Draw Things"
    ]
  },
  {
    "objectID": "6381Lab07.html",
    "href": "6381Lab07.html",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "",
    "text": "This lab introduces spatial selection techniques in ArcGIS, focusing on proximity and adjacency. You will learn how to import and manipulate tables, perform joins, and create summary statistics. The lab also emphasizes the importance of repeating tasks to master GIS skills.",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab07.html#introduction",
    "href": "6381Lab07.html#introduction",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "",
    "text": "This lab introduces spatial selection techniques in ArcGIS, focusing on proximity and adjacency. You will learn how to import and manipulate tables, perform joins, and create summary statistics. The lab also emphasizes the importance of repeating tasks to master GIS skills.",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab07.html#objectives",
    "href": "6381Lab07.html#objectives",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nLearn to select features based on proximity and adjacency.\nPractice importing tables and performing joins.\nCreate maps that summarize spatial data using tables.",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab07.html#tasks",
    "href": "6381Lab07.html#tasks",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Select by Proximity and Adjacency\n\nUse spatial selection tools to find features based on their proximity to other features.\nExplore the relationship between different geographic features by selecting adjacent polygons.\n\n\n\n3.2 2. Import Tables and Perform Joins\n\nImport an Excel table, summarize it, and then join it with a shapefile.\nLearn how to handle common issues like missing keys or multiple entries.",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab07.html#results",
    "href": "6381Lab07.html#results",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 US Corn Production by County\n\n\n\nUS Corn Production by County\n\n\n\n\n4.2 California County Income and Recreation Maps\n\n\n\nCalifornia County Income",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab07.html#conclusion",
    "href": "6381Lab07.html#conclusion",
    "title": "Lab 07: Spatial Selection in ArcGIS",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab provided hands-on experience with spatial selection and table manipulation in ArcGIS. The tasks involved selecting features based on proximity and adjacency, importing and joining tables, and creating maps that summarize spatial data. By completing these exercises, you’ve gained a deeper understanding of spatial relationships and how to use ArcGIS to analyze them.",
    "crumbs": [
      "ArcGIS",
      "Lab 07: Spatial Selection in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html",
    "href": "6381Lab06.html",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "",
    "text": "This lab focuses on tabular data management in ArcGIS. It involves viewing, selecting, reordering, and updating tabular data. The lab uses the USCounties.shp data layer and the soils.shp data set for various tasks.",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html#introduction",
    "href": "6381Lab06.html#introduction",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "",
    "text": "This lab focuses on tabular data management in ArcGIS. It involves viewing, selecting, reordering, and updating tabular data. The lab uses the USCounties.shp data layer and the soils.shp data set for various tasks.",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html#objectives",
    "href": "6381Lab06.html#objectives",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nPractice tabular data management in ArcGIS.\nLearn how to create and join tables.",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html#tasks",
    "href": "6381Lab06.html#tasks",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Selecting by Attribute\nExplore and select features using the Select by Attributes tool in ArcGIS. For example, create a map displaying burglary rates for each county and normalize these rates by the population.\n\n\n3.2 2. Creating and Joining Tables\nCreate a new table containing soil properties, then join this table with the soils data layer. The joined table will be used to create a layout showing soils by fertility class.",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html#results",
    "href": "6381Lab06.html#results",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 High Population Counties with High Old/Young Ratios\n\n\n\nHigh Population Counties with High Old/Young Ratios\n\n\n\n\n4.2 Macon Country North Carolina Soil Fertility\n\n\n\nMacon Country North Carolina Soil Fertility\n\n\n\n\n4.3 US Counties Cow Density per Square Mile\n\n\n\nUS Counties Cow Density per Square Mile",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab06.html#conclusion",
    "href": "6381Lab06.html#conclusion",
    "title": "Lab 06: Tables in ArcGIS",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab provided hands-on experience with selecting attributes, managing tables, and joining data in ArcGIS. The ability to manipulate and display tabular data is crucial for effective GIS analysis.",
    "crumbs": [
      "ArcGIS",
      "Lab 06: Tables in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab10.html",
    "href": "6381Lab10.html",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "",
    "text": "In this lab, we explore the use of Landsat 9 imagery for remote sensing data analysis. We will focus on downloading, displaying, and analyzing Landsat data using ArcGIS Pro, including creating composite images and conducting change detection analysis.",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab10.html#introduction",
    "href": "6381Lab10.html#introduction",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "",
    "text": "In this lab, we explore the use of Landsat 9 imagery for remote sensing data analysis. We will focus on downloading, displaying, and analyzing Landsat data using ArcGIS Pro, including creating composite images and conducting change detection analysis.",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab10.html#objectives",
    "href": "6381Lab10.html#objectives",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nLearn how to download and process Landsat 9 imagery.\nUnderstand the methods for displaying and analyzing remote sensing data in ArcGIS Pro.\nCreate composite images using multiple Landsat bands.\nPerform change detection analysis to assess land cover changes over time.",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab10.html#tasks",
    "href": "6381Lab10.html#tasks",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Downloading Landsat Imagery\nUsing the Earth Explorer tool, download Landsat 9 imagery for a region of interest. For this lab, the target area is Dallas, Texas.\n\n\n3.2 2. Displaying Landsat Imagery in ArcGIS Pro\nLoad the downloaded Landsat 9 imagery into ArcGIS Pro and explore various methods for displaying and analyzing the data. Specifically, create a composite image using the different spectral bands provided by the Landsat dataset.\n\n\n3.3 3. Creating a Composite Image\nUtilize the geoprocessing tools in ArcGIS Pro to combine the individual Landsat bands into a single composite image. The composite image will allow us to visualize the area using different spectral bands and understand the various land cover features.\n\n\n3.4 4. Change Detection Analysis\nConduct a change detection analysis using Landsat imagery from 2014 and 2024 for the Dallas area. The goal is to identify land cover changes over the past decade. The change detection process will involve comparing specific spectral bands from the two different time periods.",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab10.html#results",
    "href": "6381Lab10.html#results",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "4 Results",
    "text": "4 Results\n\n4.0.1 Map: Adding and Exploring Landsat Imagery in ArcGIS Pro\n\n\n\nAdding and Exploring Landsat Imagery in ArcGIS Pro\n\n\n\n\n4.0.2 Sceenshot: Composite Image\n\n\n\nComposite Image\n\n\n\n\n4.0.3 Map: Change Detection Analysis\n\n\n\nChange Detection",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab10.html#conclusion",
    "href": "6381Lab10.html#conclusion",
    "title": "Lab 10: Remote Sensing Data Analysis",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThe analysis performed in this lab demonstrates the capabilities of Landsat 9 imagery in remote sensing applications. By creating composite images and conducting change detection, we were able to visualize and quantify land cover changes in the Dallas area over the past decade. These techniques are crucial for monitoring environmental changes and informing decision-making processes.",
    "crumbs": [
      "ArcGIS",
      "Lab 10: Remote Sensing Data Analysis"
    ]
  },
  {
    "objectID": "6381Lab05.html",
    "href": "6381Lab05.html",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "",
    "text": "This lab focuses on the use of digital data and performing basic table operations in GIS. It involves manipulating different data sets and creating maps that represent various geographic and population-related data.",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "6381Lab05.html#introduction",
    "href": "6381Lab05.html#introduction",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "",
    "text": "This lab focuses on the use of digital data and performing basic table operations in GIS. It involves manipulating different data sets and creating maps that represent various geographic and population-related data.",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "6381Lab05.html#objectives",
    "href": "6381Lab05.html#objectives",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nExplore different types of digital data sets.\nPerform basic table operations in ArcGIS.\nCreate and interpret thematic maps.",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "6381Lab05.html#tasks",
    "href": "6381Lab05.html#tasks",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Kodiak Island Cities\nA map was created using USGS data to identify cities on Kodiak Island, Alaska. The cities were selected and saved as a subset.\n\n\n3.2 2. Population Density Map\nUsing U.S. census data, a population density map was generated, highlighting the varying population densities across different regions.\n\n\n3.3 3. County Population Symbol Map\nA proportional symbol map was created to display county population for the lower 48 U.S. states.\n\n\n3.4 4. Shaded Relief and Hydrography Map\nDigital elevation and hydrological data were utilized to create a shaded-relief map, enhancing the visual representation of the terrain.\n\n\n3.5 5. Wetlands by Size Class\nWetlands data were classified by size, and a map was produced to display the various size classes of wetlands.",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "6381Lab05.html#results",
    "href": "6381Lab05.html#results",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "4 Results",
    "text": "4 Results\n\n4.0.1 Central Minnesota Population\n\n\n\nCentral Minnesota Population\n\n\n\n\n4.0.2 Lower St. Croix Watershed\n\n\n\nLower St. Croix Watershed\n\n\n\n\n4.0.3 Kodiak Island Cities\n\n\n\nKodiak Island Cities\n\n\n\n\n4.0.4 U.S. Census Data\n\n\n\nU.S. Census Data\n\n\n\n\n4.0.5 Stillwater Wetlands by Size\n\n\n\nStillwater Wetlands by Size",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "6381Lab05.html#conclusion",
    "href": "6381Lab05.html#conclusion",
    "title": "Lab 05: Digital Data and Table Operations",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab provided practical experience in handling various digital data sets and performing basic table operations in ArcGIS. Through the creation of multiple thematic maps, key skills in data manipulation and visualization were developed, which are essential for effective spatial analysis.",
    "crumbs": [
      "ArcGIS",
      "Lab 05: Digital Data and Table Operations"
    ]
  },
  {
    "objectID": "Product_Rendering_with_AI.html",
    "href": "Product_Rendering_with_AI.html",
    "title": "5. Product Rendering with AI",
    "section": "",
    "text": "This workflow is inspired by the tutorial from 惫懒の欧阳川. We’ll demonstrate how to use ComfyUI, ControlNet, and IPAdapter to create high-quality product renders. AI-driven product rendering allows you to generate realistic product images with minimal manual input, making it ideal for e-commerce, advertising, and product design.\nBefore diving into the workflow, we’d like to introduce Shakker, a platform offering a wide range of commercially usable models and assets. Models purchased from Shakker are licensed for commercial use, making it a valuable resource for product designers and 3D artists looking to integrate professional models into their projects.\nIn this tutorial, we’ll focus on enhancing a backpack’s line art using ControlNet. The process starts by using LineArt ControlNet to define the structure, followed by applying Soft Edge Line to refine and strengthen the edges. Once the line art is complete, we’ll demonstrate a simple texture modification using positive condition keywords. Finally, we’ll apply a more advanced material change using an RGB mask, where IPAdapter selectively modifies the texture of the red areas in the design, allowing for precise control over specific elements of the product’s appearance.\nThe complete workflow is available below, and you can import the JSON file into ComfyUI by dragging it into the interface.\nDownload the workflow JSON file\n\nRendered Product Example:",
    "crumbs": [
      "AI Art & Animation",
      "5. Product Rendering with AI"
    ]
  },
  {
    "objectID": "Product_Rendering_with_AI.html#introduction",
    "href": "Product_Rendering_with_AI.html#introduction",
    "title": "5. Product Rendering with AI",
    "section": "",
    "text": "This workflow is inspired by the tutorial from 惫懒の欧阳川. We’ll demonstrate how to use ComfyUI, ControlNet, and IPAdapter to create high-quality product renders. AI-driven product rendering allows you to generate realistic product images with minimal manual input, making it ideal for e-commerce, advertising, and product design.\nBefore diving into the workflow, we’d like to introduce Shakker, a platform offering a wide range of commercially usable models and assets. Models purchased from Shakker are licensed for commercial use, making it a valuable resource for product designers and 3D artists looking to integrate professional models into their projects.\nIn this tutorial, we’ll focus on enhancing a backpack’s line art using ControlNet. The process starts by using LineArt ControlNet to define the structure, followed by applying Soft Edge Line to refine and strengthen the edges. Once the line art is complete, we’ll demonstrate a simple texture modification using positive condition keywords. Finally, we’ll apply a more advanced material change using an RGB mask, where IPAdapter selectively modifies the texture of the red areas in the design, allowing for precise control over specific elements of the product’s appearance.\nThe complete workflow is available below, and you can import the JSON file into ComfyUI by dragging it into the interface.\nDownload the workflow JSON file\n\nRendered Product Example:",
    "crumbs": [
      "AI Art & Animation",
      "5. Product Rendering with AI"
    ]
  },
  {
    "objectID": "Product_Rendering_with_AI.html#workflow-overview",
    "href": "Product_Rendering_with_AI.html#workflow-overview",
    "title": "5. Product Rendering with AI",
    "section": "2 Workflow Overview",
    "text": "2 Workflow Overview\n\nWorkflow Setup: \n\nBefore diving into the more complex product rendering process, it’s essential to first set up a basic ComfyUI workflow using positive and negative keywords. This step helps create a foundational product render by providing clear descriptions of what the product should look like and eliminating any unwanted elements.\n\nPositive Keywords:\n\n(masterpiece, best quality, highres:1.0), Exquisite gifts, product design, zipper, backpack, simple background, booth_No no_human, Simple decoration\n\nNegative Keywords:\n\nembedding: EasyNavigate, (blur, Low Res:1.2), gold, \nThis basic workflow will generate an initial render of the product based on your keywords.\n\nBasic Workflow: \n\n\n2.1 Step 1: Line Art Extraction with ControlNet\nThe design for the backpack is prepared beforehand. In this step, we use the LineArt ControlNet model to extract the line art from the pre-existing design. This helps isolate the key structural elements of the backpack, ensuring that the design lines are clean and ready for further modifications.\n\nBag Sample:\n\n\n\nLineArt ControlNet Example: \n\n\n\n2.2 Step 2: Strengthening Line Edges with Soft Edge Line\nAfter generating the line art, the Soft Edge Line technique is applied to deepen and refine the line edges, giving the image a more defined and professional look. This step enhances the overall clarity of the design, making the details stand out.\n\nSoft Edge Line Example:\n\nSoft Edge Line Workflow:\n\nMultiple ControlNet and Soft Edge Line Workflow:\n\n\n\n\n2.3 Step 3: Texture Modification with Positive Conditions\nOnce the line art is complete, we demonstrate a simple texture modification using positive condition keywords. These keywords describe the material or texture you wish to apply to the backpack. For example, “leather” or “canvas” can be used to generate different textures.\n\nLeather Texture Example:\n\nCanvas Texture Example:\n\n\n\n\n2.4 Step 4: Advanced Material Changes with RGB Mask and IPAdapter\nIn this step, the RGB mask is created using Photoshop to isolate different parts of the product for specific material alterations. The Bag Sample is converted into an RGB image, where different sections (like straps, pockets, and body) are assigned distinct colors (red, green, blue). This allows precise control over which parts of the product will receive material changes.\nBefore applying textures, it is recommended to use a color tool to inspect the color range and ensure accurate targeting of the channels (red, green, blue). This helps avoid misalignment when applying textures to specific parts of the product.\n\nRGB Mask Example:\n\n\nOnce the RGB mask is created, the red channel is isolated and used to apply a new texture to the selected area of the backpack using IPAdapter.\n\nMask Preview:\n\n\nFinally, the IPAdapter is applied to modify the texture of the red area, giving the backpack a crocodile skin effect, while leaving the green and blue areas unchanged.\n\nMaterial Applied to Mask:\n\n\n\n\n2.5 Step 5: Final Render and Material Comparison\nIn this step, we show the final product render using different materials and demonstrate how adjusting the weight of each material influences the final outcome. Below, you’ll see the original material images followed by the renders that incorporate these materials. This comparison allows you to better understand the impact of texture and weight in product rendering.\n\nMaterial 1: Crocodile Skin\n\nFinal Render with Crocodile Skin (Weight 0.3)\n\nFinal Render with Crocodile Skin (Weight 0.5)\n\n\n\n\nMaterial 2: Distressed Fabric\n\nFinal Render with Distressed Fabric (Weight 0.6)\n\n\nBy adjusting the weight of each material, we can achieve varying levels of influence on the product’s texture, allowing for precise control over the final appearance.",
    "crumbs": [
      "AI Art & Animation",
      "5. Product Rendering with AI"
    ]
  },
  {
    "objectID": "Product_Rendering_with_AI.html#conclusion",
    "href": "Product_Rendering_with_AI.html#conclusion",
    "title": "5. Product Rendering with AI",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nIn this workflow, we’ve demonstrated how to leverage ComfyUI, ControlNet, and IPAdapter to create high-quality product renders. By combining multiple steps, such as enhancing line art, applying texture modifications through positive condition keywords, and using advanced techniques like RGB masking, we’ve shown how AI-driven tools can give you precise control over specific design elements.\nThis process is highly versatile, making it ideal for industries like e-commerce, advertising, and product design where high-quality visuals are essential. By adjusting weights and materials, you can further refine your product’s final appearance, ensuring that each render meets your aesthetic goals.\nWhile this workflow focused on using the red channel to modify textures, in practice, multiple channels can be utilized to apply different materials to various parts of a product. This technique allows for more precise control and customization, enabling designers to apply specific textures to different regions, creating a more intricate and detailed final render.\nExperimenting with different textures and weights allows for a wide variety of outcomes, demonstrating the power of AI in creating customizable and professional product renders with minimal manual input.",
    "crumbs": [
      "AI Art & Animation",
      "5. Product Rendering with AI"
    ]
  },
  {
    "objectID": "vid2vid.html",
    "href": "vid2vid.html",
    "title": "4. Video to Video Animation Workflow",
    "section": "",
    "text": "In this workflow, we transform an original video of a cheerleading girl Origin Video into an anime-style animation of a medieval fantasy woman casting fire magic in a forest. We use ControlNet, Mask, IPAdapter, and AnimateDiff to achieve this transformation. The process involves taking a base video and applying frame-by-frame transformation techniques to enhance and modify the visual elements of the video.\nAfter the animation undergoes one round of upscaling, we utilize the Face Detailer block to refine the character’s facial details by reading the character’s skeleton from the upscaled animation. This allows us to generate a Mask Animation, where we repaint the masked facial areas to achieve a cleaner and more detailed animation.\nThe complete workflow can be found below for reference and can be imported into ComfyUI for further adjustments.\nDownload the workflow JSON file\n\nCasting Spell Animation Output_Face Detailer:\n\n\n\n\nOutput_Face Detailer\n\n\n\nComparison: Output Video vs. Original:\n\n\n\n\nCompare",
    "crumbs": [
      "AI Art & Animation",
      "4. Video to Video Animation Workflow"
    ]
  },
  {
    "objectID": "vid2vid.html#introduction",
    "href": "vid2vid.html#introduction",
    "title": "4. Video to Video Animation Workflow",
    "section": "",
    "text": "In this workflow, we transform an original video of a cheerleading girl Origin Video into an anime-style animation of a medieval fantasy woman casting fire magic in a forest. We use ControlNet, Mask, IPAdapter, and AnimateDiff to achieve this transformation. The process involves taking a base video and applying frame-by-frame transformation techniques to enhance and modify the visual elements of the video.\nAfter the animation undergoes one round of upscaling, we utilize the Face Detailer block to refine the character’s facial details by reading the character’s skeleton from the upscaled animation. This allows us to generate a Mask Animation, where we repaint the masked facial areas to achieve a cleaner and more detailed animation.\nThe complete workflow can be found below for reference and can be imported into ComfyUI for further adjustments.\nDownload the workflow JSON file\n\nCasting Spell Animation Output_Face Detailer:\n\n\n\n\nOutput_Face Detailer\n\n\n\nComparison: Output Video vs. Original:\n\n\n\n\nCompare",
    "crumbs": [
      "AI Art & Animation",
      "4. Video to Video Animation Workflow"
    ]
  },
  {
    "objectID": "vid2vid.html#workflow-overview",
    "href": "vid2vid.html#workflow-overview",
    "title": "4. Video to Video Animation Workflow",
    "section": "2 Workflow Overview",
    "text": "2 Workflow Overview\nThis project is adapted from Author Akumetsu971\n\nWorkflow Setup: \n\n\n2.1 Step 1: Input Video\nFirst, import the original video and set the positive and negative keywords describing the video you wish to create. In this step, you can also configure the total frame count and how many frames to skip between each one. Since most original videos have a high frame rate, processing this many frames can consume large amounts of memory and time. It’s important to first assess the quality, resolution, and specific areas that require attention to optimize the workload.\n\nLoad Video: \n\n\n\n2.2 Step 2: ControlNet Configuration\nIn this step, we configure ControlNet to manage multiple aspects of the animation, including depth, edge detection, line art, and pose estimation. Each ControlNet model is assigned to a specific task, such as detecting depth or extracting key facial points. By using a combination of models, we can fine-tune the details of the animation and ensure that each element of the video is correctly processed.\nWhen ControlNet is not applied, key details may be lost, leading to less precise and dynamic animations. Below is an example comparing the results of a video without ControlNet configurations.\n\nControlNet Workflow Setup: \nExample video without ControlNet:\n  Your browser does not support the video tag. \n\n\n\n2.3 Step 3: Mask and IPAdapter Background\nIn this step, we apply the mask concept mentioned in the previous section, filtering elements that are essential for the animation. Simultaneously, we use IPAdapter to define the foreground and background of the scene. In this example, the foreground is a medieval fantasy-style woman, and the background is a forest environment. The Mask allows us to isolate and control these elements more effectively, making it possible to modify only the foreground character or the background independently. By adjusting the weight and timing of these elements through IPAdapter, we create a balanced and dynamic animation where the character interacts with the environment seamlessly.\n\nMask and IPAdapter: \n\n\n\n2.4 Step 4: AnimateDiff Application\nIn this step, we apply AnimateDiff to transform the video frames into a consistent animation style. AnimateDiff is responsible for blending the different elements together, such as the masked foreground and background images, while maintaining smooth transitions between frames. By adjusting the strength of the AnimateDiff model, we can control the overall look of the animation, such as the degree of stylization and the level of detail retained from the original video.\n\nAnimateDiff: \n\n\n\n2.5 Step 5: Upscaling and Face Detail\nAfter the initial animation has been generated, we apply Upscaling to improve the overall resolution and quality of the animation. Upscaling allows us to increase the clarity and definition of the details, making the final animation look sharper and more refined.\nOnce the animation has been upscaled, we use the Face Detailer module to focus specifically on the character’s facial features. This step is crucial to ensure that the face is not blurred or distorted during the upscaling process. The Face Detailer reads the character’s skeleton and key facial points, using them to generate a mask that isolates the face for more detailed rendering. This mask is then used to repaint the facial area with enhanced detail, ensuring a clear and accurate depiction of facial expressions in each frame.\nBy refining the face and ensuring high-quality results, this step elevates the overall aesthetic of the animation, making the character’s appearance more lifelike and visually appealing.\n\nFace Detail: \n\n\n\n\nOutput_Up Scale\n\n\n\n\n\nOutput_Face Detailer\n\n\n\n\n2.6 Step 6: Final Video Combination\nFinally, after completing all the previous steps of generating frames, upscaling, and refining facial details, we use the Video Combine node(KSampler) to merge all the frames into a single cohesive video file.\nThis step ensures that all elements — such as the refined face, detailed background, and overall animation — are synchronized and seamlessly combined into the final animation video.\n\nCompare result:",
    "crumbs": [
      "AI Art & Animation",
      "4. Video to Video Animation Workflow"
    ]
  },
  {
    "objectID": "vid2vid.html#conclusion",
    "href": "vid2vid.html#conclusion",
    "title": "4. Video to Video Animation Workflow",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nIn this workflow, we successfully transformed a cheerleading video into an anime-style animation of a character casting fire magic, using advanced techniques with ControlNet, IPAdapter, AnimateDiff, and Face Detailer. By leveraging these powerful tools, we were able to fine-tune the animation and enhance key elements like the character’s face, background, and overall animation flow.\nThe ability to combine masks, adjust motion scales, and use Face Detailer for finer facial features allowed for an impressive level of detail in the final output. Each step, from creating masks to applying the final upscaling, contributed to a smooth and polished result.\nWith this workflow, users can explore different styles and approaches to video-to-video transformations, experimenting with various control nodes and configurations to fit their unique creative needs.",
    "crumbs": [
      "AI Art & Animation",
      "4. Video to Video Animation Workflow"
    ]
  },
  {
    "objectID": "6381Lab03.html",
    "href": "6381Lab03.html",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "",
    "text": "This lab focuses on the processes of manual and on-screen digitizing within ArcGIS Pro. The primary goal is to digitize features from aerial imagery and other source media, creating accurate vector data layers.",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "6381Lab03.html#introduction",
    "href": "6381Lab03.html#introduction",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "",
    "text": "This lab focuses on the processes of manual and on-screen digitizing within ArcGIS Pro. The primary goal is to digitize features from aerial imagery and other source media, creating accurate vector data layers.",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "6381Lab03.html#objectives",
    "href": "6381Lab03.html#objectives",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nDigitize features such as buildings, roads, and ponds using ArcGIS Pro.\nUtilize snapping tools to ensure precision in editing.\nSave and export digitized features in map formats.",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "6381Lab03.html#tasks",
    "href": "6381Lab03.html#tasks",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Digitize Landcover and Subdivision Polygons\nUse aerial imagery as the base layer to digitize landcover and new subdivision polygons. Apply snapping tools to ensure that all features are correctly aligned.\n\n\n3.2 2. Digitize MAP Township Features\nTrace and digitize features such as buildings, roads, and ponds in MAP Township. Ensure each feature is accurately represented in the vector data layer.",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "6381Lab03.html#results",
    "href": "6381Lab03.html#results",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Updated Landcover and Subdivision Polygons\n\n\n\nEditing Practice\n\n\n\n\n4.2 MAP Township Digitized Features\n\n\n\nMAP Township",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "6381Lab03.html#conclusion",
    "href": "6381Lab03.html#conclusion",
    "title": "Lab 03: Data Entry and Digitizing",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nDigitizing features from aerial imagery allows for the accurate creation of vector data layers, essential for many GIS applications. Proper use of snapping tools ensures that digitized features are precise, which is critical in producing reliable spatial data.",
    "crumbs": [
      "ArcGIS",
      "Lab 03: Data Entry and Digitizing"
    ]
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Jimpan's Knowledge Hub",
    "section": "",
    "text": "You have come to the wasteland of knowledge, which will be updated soon.\n\n\n\n Back to top",
    "crumbs": [
      "assign01.html"
    ]
  },
  {
    "objectID": "arcgis.html",
    "href": "arcgis.html",
    "title": "ArcGIS Labs Overview",
    "section": "",
    "text": "Please use sidebar to view labs.\nWelcome to the ArcGIS Labs section of my project. This page serves as an overview and gateway to the various labs conducted using ArcGIS, a powerful tool for geographic information system (GIS) analysis. These labs are designed to enhance my skills in spatial data analysis, cartography, and remote sensing.\n\n\n\n\n\nLayout Sample",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "arcgis.html#introduction",
    "href": "arcgis.html#introduction",
    "title": "ArcGIS Labs Overview",
    "section": "",
    "text": "Please use sidebar to view labs.\nWelcome to the ArcGIS Labs section of my project. This page serves as an overview and gateway to the various labs conducted using ArcGIS, a powerful tool for geographic information system (GIS) analysis. These labs are designed to enhance my skills in spatial data analysis, cartography, and remote sensing.\n\n\n\n\n\nLayout Sample",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "arcgis.html#objectives",
    "href": "arcgis.html#objectives",
    "title": "ArcGIS Labs Overview",
    "section": "2 Objectives",
    "text": "2 Objectives\nThe primary objectives of these labs are:\n\nTo gain hands-on experience with ArcGIS software.\nTo develop proficiency in various GIS analysis techniques, including spatial analysis, raster processing, and remote sensing.\nTo apply these techniques to real-world scenarios, such as land cover analysis, population studies, and environmental monitoring.",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "arcgis.html#lab-summaries",
    "href": "arcgis.html#lab-summaries",
    "title": "ArcGIS Labs Overview",
    "section": "3 Lab Summaries",
    "text": "3 Lab Summaries\n\n3.1 Lab 01: Introduction to ArcGIS\nAn introductory lab focusing on getting familiar with the ArcGIS interface and basic functions. Topics include map navigation, layer management, and simple spatial queries.\n\n\n3.2 Lab 02: Projections in ArcGIS\nThis lab covers the importance of map projections and how to work with different coordinate systems in ArcGIS. You will learn to project spatial data accurately and analyze how projections impact spatial analysis.\n\n\n3.3 Lab 03: Data Entry and Editing\nThis lab teaches how to digitize spatial features, edit attributes, and manage spatial data effectively. It’s a crucial skill for maintaining accurate and up-to-date GIS datasets.\n\n\n3.4 Lab 04: Digitizing and Topology\nExplore advanced digitizing techniques and learn about the role of topology in maintaining spatial data integrity. The lab includes practical exercises in creating and correcting topological errors.\n\n\n3.5 Lab 05: Working with Digital Data and Tables\nLearn how to import, manage, and analyze tabular data within ArcGIS. This lab focuses on linking spatial data with attribute tables and performing table-based queries.\n\n\n3.6 Lab 06: Advanced Table Operations\nThis lab delves into more complex table operations, including joins, relates, and summarization techniques, which are essential for in-depth spatial analysis.\n\n\n3.7 Lab 07: Spatial Selection and Queries\nLearn to perform spatial queries and selections based on various criteria. This lab demonstrates how to extract meaningful information from large spatial datasets.\n\n\n3.8 Lab 08: Buffering and Overlay Analysis\nThis lab covers buffering and overlay techniques to analyze spatial relationships. It’s commonly used in environmental impact studies and site suitability analyses.\n\n\n3.9 Lab 09: Raster Data Analysis\nExplore raster data processing techniques, including classification, reclassification, and raster algebra. The lab emphasizes the use of raster data for land cover and elevation analysis.\n\n\n3.10 Lab 10: Remote Sensing Data\nThis lab focuses on remote sensing applications, including the use of Landsat imagery for land cover change detection and environmental monitoring.",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "arcgis.html#conclusion",
    "href": "arcgis.html#conclusion",
    "title": "ArcGIS Labs Overview",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThese labs represent a comprehensive journey through the functionalities of ArcGIS, from basic operations to advanced spatial analysis techniques. Each lab builds on the previous ones, progressively expanding the range of skills and knowledge in GIS. Feel free to explore each lab in detail to see the results and methodologies used.",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "arcgis.html#next-steps",
    "href": "arcgis.html#next-steps",
    "title": "ArcGIS Labs Overview",
    "section": "5 Next Steps",
    "text": "5 Next Steps\nAs I continue to learn and apply GIS techniques, I will be adding more labs and projects to this section. Stay tuned for updates!",
    "crumbs": [
      "ArcGIS",
      "ArcGIS Labs Overview"
    ]
  },
  {
    "objectID": "6381Lab08.html",
    "href": "6381Lab08.html",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "",
    "text": "In this lab, we will explore spatial analysis techniques using buffering and overlay operations in ArcGIS. These techniques are fundamental in geographic information systems (GIS) and are often used to determine spatial relationships between various features.",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6381Lab08.html#introduction",
    "href": "6381Lab08.html#introduction",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "",
    "text": "In this lab, we will explore spatial analysis techniques using buffering and overlay operations in ArcGIS. These techniques are fundamental in geographic information systems (GIS) and are often used to determine spatial relationships between various features.",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6381Lab08.html#objectives",
    "href": "6381Lab08.html#objectives",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nApply the concepts of buffering and overlay in GIS.\nCreate maps demonstrating buffer zones and suitable areas for specific land use.\nPerform spatial analysis to identify potential campgrounds based on proximity to lakes and roads.",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6381Lab08.html#tasks",
    "href": "6381Lab08.html#tasks",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Create Buffer Zones\nUsing the provided data (lakes.shp, roads.shp), generate buffer zones around lakes and roads. Apply variable distance buffers for lakes based on their size and fixed distance buffers for roads.\n\n\n3.2 2. Perform Overlay Analysis\nOverlay the buffer zones to identify areas that meet both the criteria for being near a lake and a road. Use the union operation to combine these layers and identify suitable areas for potential campgrounds.\n\n\n3.3 3. Analyze Suitable Areas\nDetermine the size of the suitable areas identified in the overlay analysis. Calculate the area in hectares and create maps that display these areas.",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6381Lab08.html#results",
    "href": "6381Lab08.html#results",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Hugo Minnesota Lakes and Roads - Buffer Zones\n\n\n\nBuffer Zones\n\n\n\n\n4.2 Hugo Minnesota Lakes and Roads - Overlay Analysis\n\n\n\nOverlay Analysis\n\n\n\n\n4.3 Hugo Minnesota Lakes and Roads - Campground Locations\n\n\n\nCampground Locations",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6381Lab08.html#conclusion",
    "href": "6381Lab08.html#conclusion",
    "title": "Lab 08: Spatial Analysis Buffering and Overlay",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab demonstrated the use of buffering and overlay operations in GIS to identify suitable areas for campgrounds. By applying these spatial analysis techniques, we can make informed decisions about land use based on proximity to key features such as lakes and roads.",
    "crumbs": [
      "ArcGIS",
      "Lab 08: Spatial Analysis Buffering and Overlay"
    ]
  },
  {
    "objectID": "6323Lab03.html",
    "href": "6323Lab03.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "1 R Programming (EDA)\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\n2 Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EPPS 6323: Lab03"
    ]
  },
  {
    "objectID": "comfyui.html",
    "href": "comfyui.html",
    "title": "ComfyUI",
    "section": "",
    "text": "Please use sidebar to view workflows.\nComfyUI is a modular, node-based interface for AI-powered image and animation generation. It allows users to have granular control over each step of the creative process, from model selection to image output. ComfyUI is designed to provide both flexibility and precision, making it a great tool for both beginners and advanced users.\nInstall ComfyUI tutorial on MacOS\n\n\n\n\n\nWorkflow Sample\n\n\n\n\n\n\n\n\nWorkflow Sample2\n\n\n\n\n\n\nNode-based Workflow: ComfyUI’s interface is built around connecting nodes, each representing a different function or action in the image generation process. This allows for customized workflows to suit specific project requirements.\nAI Model Support: ComfyUI integrates with popular AI models, including Stable Diffusion and ControlNet, offering a broad range of image generation capabilities like text-to-image, image-to-image, and more.\nAdvanced Control: Users can modify and control various parameters such as sampling methods, resolutions, and image styles, providing detailed customization for output refinement.\n\n\n\n\n\nCreative Flexibility: ComfyUI’s open-ended design encourages experimentation, letting users craft complex workflows tailored to their specific creative needs.\nDetailed Customization: Adjust key aspects of the workflow to fine-tune the results, including models, prompts, and even the post-processing stages.\nIntegration with Other AI Tools: Seamless integration with models like ControlNet enables precise control over elements like poses, faces, and more within generated images.\n\n\n\n\n\nText-to-Image: Input a prompt and generate images using models like Stable Diffusion. Modify the process using various nodes to get the exact style or detail you need.\nImage Modification: Generate variations of existing images by using nodes that apply transformations or post-processing filters to enhance or change the image.\nAnimation Creation: By linking multiple image outputs and nodes, ComfyUI can be used to create dynamic animations with controlled transitions between frames.\n\n\n\n\nFor more information, updates, or to contribute to the project, you can visit the official ComfyUI GitHub Repository.",
    "crumbs": [
      "AI Art & Animation",
      "ComfyUI"
    ]
  },
  {
    "objectID": "comfyui.html#key-features",
    "href": "comfyui.html#key-features",
    "title": "ComfyUI",
    "section": "",
    "text": "Node-based Workflow: ComfyUI’s interface is built around connecting nodes, each representing a different function or action in the image generation process. This allows for customized workflows to suit specific project requirements.\nAI Model Support: ComfyUI integrates with popular AI models, including Stable Diffusion and ControlNet, offering a broad range of image generation capabilities like text-to-image, image-to-image, and more.\nAdvanced Control: Users can modify and control various parameters such as sampling methods, resolutions, and image styles, providing detailed customization for output refinement.",
    "crumbs": [
      "AI Art & Animation",
      "ComfyUI"
    ]
  },
  {
    "objectID": "comfyui.html#why-use-comfyui",
    "href": "comfyui.html#why-use-comfyui",
    "title": "ComfyUI",
    "section": "",
    "text": "Creative Flexibility: ComfyUI’s open-ended design encourages experimentation, letting users craft complex workflows tailored to their specific creative needs.\nDetailed Customization: Adjust key aspects of the workflow to fine-tune the results, including models, prompts, and even the post-processing stages.\nIntegration with Other AI Tools: Seamless integration with models like ControlNet enables precise control over elements like poses, faces, and more within generated images.",
    "crumbs": [
      "AI Art & Animation",
      "ComfyUI"
    ]
  },
  {
    "objectID": "comfyui.html#how-it-works",
    "href": "comfyui.html#how-it-works",
    "title": "ComfyUI",
    "section": "",
    "text": "Text-to-Image: Input a prompt and generate images using models like Stable Diffusion. Modify the process using various nodes to get the exact style or detail you need.\nImage Modification: Generate variations of existing images by using nodes that apply transformations or post-processing filters to enhance or change the image.\nAnimation Creation: By linking multiple image outputs and nodes, ComfyUI can be used to create dynamic animations with controlled transitions between frames.",
    "crumbs": [
      "AI Art & Animation",
      "ComfyUI"
    ]
  },
  {
    "objectID": "comfyui.html#official-github-repository",
    "href": "comfyui.html#official-github-repository",
    "title": "ComfyUI",
    "section": "",
    "text": "For more information, updates, or to contribute to the project, you can visit the official ComfyUI GitHub Repository.",
    "crumbs": [
      "AI Art & Animation",
      "ComfyUI"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Jimpan's Knowledge Hub",
    "section": "",
    "text": "You have come to the wasteland of knowledge, which will be updated soon. Or you can head to my GitHub for detailed projects.\n\n\n\n Back to top"
  },
  {
    "objectID": "6323Lab02.html",
    "href": "6323Lab02.html",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into '/Users/jimpan/Library/R/arm64/4.3/library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'MASS' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\n\nThe downloaded binary packages are in\n    /var/folders/m3/k788kw6103zdvc0bwpc1lzd00000gn/T//Rtmp021unB/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#indexing-data-using",
    "href": "6323Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#loading-data-from-github",
    "href": "6323Lab02.html#loading-data-from-github",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#load-data-from-islr-website",
    "href": "6323Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "6323Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#linear-regression",
    "href": "6323Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into '/Users/jimpan/Library/R/arm64/4.3/library'\n(as 'lib' is unspecified)\n\n\nWarning: package 'MASS' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\n\nThe downloaded binary packages are in\n    /var/folders/m3/k788kw6103zdvc0bwpc1lzd00000gn/T//Rtmp021unB/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#multiple-linear-regression",
    "href": "6323Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "6323Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#qualitative-predictors",
    "href": "6323Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6323Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "6323Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "EPPS 6323: Lab02"
    ]
  },
  {
    "objectID": "6381Lab09.html",
    "href": "6381Lab09.html",
    "title": "Lab 09: Raster Analysis",
    "section": "",
    "text": "This lab focuses on spatial analysis and modeling using raster data. The goal is to estimate access costs across a landscape based on factors like slope and distance to roads. The process includes raster resampling, combining DEMs, filtering data, and creating cost surfaces. The lab also includes a project on correcting DEMs and generating hillshade maps.",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "6381Lab09.html#introduction",
    "href": "6381Lab09.html#introduction",
    "title": "Lab 09: Raster Analysis",
    "section": "",
    "text": "This lab focuses on spatial analysis and modeling using raster data. The goal is to estimate access costs across a landscape based on factors like slope and distance to roads. The process includes raster resampling, combining DEMs, filtering data, and creating cost surfaces. The lab also includes a project on correcting DEMs and generating hillshade maps.",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "6381Lab09.html#objectives",
    "href": "6381Lab09.html#objectives",
    "title": "Lab 09: Raster Analysis",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nLearn to mosaic raster data with different resolutions.\nApply filtering techniques to correct noisy DEM data.\nDevelop cost surfaces using slope and distance factors.\nUse raster calculator for advanced spatial analysis.",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "6381Lab09.html#tasks",
    "href": "6381Lab09.html#tasks",
    "title": "Lab 09: Raster Analysis",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Mosaic DEMs and Generate Hillshade\n\nCombine Valley3 and Valley9 DEMs, and generate hillshade maps for both.\nResample Valley9 DEM to match Valley3’s 3-meter resolution.\nCombine the two DEMs into a single dataset using raster calculator.\n\n\n\n3.2 2. Correct DEM Artifacts Using Filters\n\nApply a low-pass filter to the Shasta DEM to correct data spikes and pits.\nGenerate a new hillshade map to visualize the corrections.\nSubtract the filtered DEM from the original DEM to isolate errors, then replace erroneous cells using raster calculator.\n\n\n\n3.3 3. Develop a Cost Surface\n\nCalculate slope from DulNorthDEM and apply an exponential cost formula.\nGenerate a distance raster using the Euclidean Distance tool.\nCombine slope and distance costs into a total cost surface, applying a threshold to focus on areas under $25,000.\n\n\n\n3.4 4. Reclassification and Final Cost Calculation\n\nReclassify the total cost surface to focus on areas below $25,000.\nMultiply the reclassified raster by the total cost surface to isolate areas within budget.",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "6381Lab09.html#results",
    "href": "6381Lab09.html#results",
    "title": "Lab 09: Raster Analysis",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Combined DEM Hillshade\n\n\n\nCombined DEM Hillshade\n\n\n\n\n4.2 Filtered Shasta DEM Hillshade\n\n\n\nFiltered Shasta DEM Hillshade",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "6381Lab09.html#conclusion",
    "href": "6381Lab09.html#conclusion",
    "title": "Lab 09: Raster Analysis",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab provided hands-on experience with advanced raster data processing techniques. The tasks included data resampling, DEM correction using filters, and cost surface generation. These techniques are crucial for spatial analysis in GIS, enabling more accurate modeling and decision-making based on geographic data.",
    "crumbs": [
      "ArcGIS",
      "Lab 09: Raster Analysis"
    ]
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Jimpan's Knowledge Hub",
    "section": "",
    "text": "You have come to the wasteland of knowledge, which will be updated soon.\n\n\n\n Back to top",
    "crumbs": [
      "assign02.html"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "",
    "text": "This site serves as a repository of my academic and professional work, with a focus on data analysis, AI-driven image processing, and geographic information systems. It showcases the key projects and learning outcomes from my work in these areas, along with detailed insights and practical applications.\nExplore my work in areas like ArcGIS, ComfyUI, and my Course Assignments. Additionally, for more information about my background and qualifications, you can visit the About Me section or download my CV.\nFor code, technical documentation, and open-source contributions, visit my GitHub, where you’ll find repositories related to my projects.\nThe navigation bar at the top will guide you through the site. Feel free to explore and discover my ongoing projects and experiments.\nIf you prefer a more neutral appearance, feel free to switch to dark mode for a cleaner, more focused reading experience."
  },
  {
    "objectID": "index.html#welcome-to-jim-pans-knowledge-hub",
    "href": "index.html#welcome-to-jim-pans-knowledge-hub",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "",
    "text": "This site serves as a repository of my academic and professional work, with a focus on data analysis, AI-driven image processing, and geographic information systems. It showcases the key projects and learning outcomes from my work in these areas, along with detailed insights and practical applications.\nExplore my work in areas like ArcGIS, ComfyUI, and my Course Assignments. Additionally, for more information about my background and qualifications, you can visit the About Me section or download my CV.\nFor code, technical documentation, and open-source contributions, visit my GitHub, where you’ll find repositories related to my projects.\nThe navigation bar at the top will guide you through the site. Feel free to explore and discover my ongoing projects and experiments.\nIf you prefer a more neutral appearance, feel free to switch to dark mode for a cleaner, more focused reading experience."
  },
  {
    "objectID": "index.html#interactive-data-analysis-example",
    "href": "index.html#interactive-data-analysis-example",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "2 Interactive Data Analysis Example",
    "text": "2 Interactive Data Analysis Example\n\nlibrary(plotly)\n\n# Generate data\nset.seed(123)\ndata &lt;- data.frame(\n  x = rnorm(30, mean = 50, sd = 10), # fewer points\n  y = rnorm(30, mean = 100, sd = 20),\n  size = abs(rnorm(30, mean = 50, sd = 25)), # more variation in bubble size\n  label = paste(\"Point\", 1:30) # Labels for each point\n)\n\n# Create a bubble plot with clearer labels and enhanced size variation\np &lt;- plot_ly(\n  data, \n  x = ~x, \n  y = ~y, \n  type = 'scatter', \n  mode = 'markers+text', \n  marker = list(\n    size = ~size, \n    color = ~x, \n    colorscale = 'Viridis', \n    showscale = TRUE\n  ),\n  text = ~label, \n  textposition = \"top center\", # Position the text\n  hoverinfo = \"text\"\n) %&gt;%\n  layout(\n    title = \"Interactive Bubble Plot (Clearer Labels and Size Variation)\",\n    xaxis = list(title = \"X Axis Label\"),\n    yaxis = list(title = \"Y Axis Label\"),\n    showlegend = FALSE\n  )\n\n# Display the plot\np\n\n\n\n\n\nPlot Description:\nThis bubble plot displays 30 points with distinct size variations and clear labels. The bubble size reflects the size variable, while colors are mapped to the x values using the Viridis color scale. Labels are placed above each bubble for better readability, making it easier to observe the relationship between the x, y, and size variables."
  },
  {
    "objectID": "index.html#five-steps-of-data-analysis",
    "href": "index.html#five-steps-of-data-analysis",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "3 FIVE STEPS OF DATA ANALYSIS",
    "text": "3 FIVE STEPS OF DATA ANALYSIS\n\n3.1 DEFINE THE TOPIC\nFirst, discuss with STAKEHOLDERS to confirm their business needs and analysis objectives. Ensure that the data you aim to collect is directly related to these goals, and gain access to the database. At this stage, also consider DATA PRIVACY and COMPLIANCE to ensure that the subsequent analysis is lawful.\n\n\n3.2 DATA COLLECTION\nBased on the analysis needs, use appropriate tools (such as SQL, APIs, or WEB SCRAPING) to collect data and download it locally for processing. During collection, pay attention to DATA INTEGRITY and ACCURACY to avoid collecting excessive irrelevant data or missing key information.\n\n\n3.3 DATA CLEANING\nUse Python’s pandas, numpy, or R’s dplyr, tidyverse tools to clean the data. This step includes filling in MISSING VALUES, removing OUTLIERS, handling DUPLICATE DATA, and standardizing data formats to ensure the quality of the data meets analysis standards.\n\n\n3.4 DATA ANALYSIS\nBased on the analysis objectives, use Python or R to perform the data analysis. Common methods include CLASSIFICATION, REGRESSION, CLUSTERING, and TIME SERIES ANALYSIS. Depending on the need, choose suitable STATISTICAL or MACHINE LEARNING MODELS, and use visualization tools such as matplotlib, seaborn (Python), or ggplot2 (R) to visualize the data and discover key trends and patterns.\n\n\n3.5 PRESENTATION OF RESULTS\nPresent the analysis results in a clear and understandable way to STAKEHOLDERS. You can use tools like TABLEAU or POWER BI to create interactive dashboards or generate charts using Python and R visualization tools such as ggplot2 and plotly. Additionally, write REPORTS summarizing the results, clearly explaining the BUSINESS OR RESEARCH IMPLICATIONS, ensuring that stakeholders can understand and take appropriate action."
  },
  {
    "objectID": "index.html#資料分析五個步驟",
    "href": "index.html#資料分析五個步驟",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "4 資料分析五個步驟",
    "text": "4 資料分析五個步驟\n\n4.1 明確主題\n首先與STAKEHOLDER進行討論，確認他們的業務需求和分析目標。確保你所要抓取的資料與這些目標具有直接的關聯性，並獲得資料庫的存取權限。在這個階段，也需要考慮資料的隱私和合規性，以保證後續分析合法合規。\n\n\n4.2 資料收集\n根據分析需求，使用合適的工具（例如SQL、API或網頁爬蟲等）來抓取資料，並下載到本地進行處理。在收集過程中，注意確保資料的完整性和準確性，避免收集到過多無關資料或遺漏關鍵資料。\n\n\n4.3 資料清理\n使用Python的pandas、numpy，或R的dplyr、tidyverse等工具來清理資料。這一步包括填補缺失值、去除異常值、處理重複資料、統一資料格式等，確保資料質量達到分析的標準。\n\n\n4.4 資料分析\n根據分析目標，使用Python或R進行資料分析，常見的分析方法包括分類（CLASSIFICATION）、回歸（REGRESSION）、聚類（CLUSTERING）、時間序列分析（TIME SERIES ANALYSIS）等。根據需求，選擇適合的統計或機器學習模型進行分析，並使用圖表工具如matplotlib、seaborn（Python），或ggplot2（R）來進行資料視覺化，幫助發現資料中的關鍵趨勢和模式。\n\n\n4.5 結果呈現\n將分析結果以易於理解的方式呈現給STAKEHOLDER。可以使用TABLEAU、POWER BI等工具製作交互式儀表板，或者使用Python、R中的資料視覺化工具（如ggplot2、plotly）生成圖表。此外，撰寫報告總結分析結果，並清晰地解釋這些結果對業務或研究的影響，確保stakeholder能夠理解並採取相應的行動。"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "5 Contact",
    "text": "5 Contact\n\nEmail: jimpan0612@gmail.com\nGitHub: Jimpan0612\nLinkedIn: Chun-Yen Pan\nPersonal Website: MyWebsite"
  },
  {
    "objectID": "index.html#update-log",
    "href": "index.html#update-log",
    "title": "Jim Pan’s Knowledge Hub",
    "section": "6 Update Log",
    "text": "6 Update Log\n\n\n6.1 September 8, 2024\n\nUpdated Homepage.\nAdded Sitemap.\nUpdated Product rendering examples Lab.\n\n\n\n6.2 September 7, 2024\n\nUpdated about me.\nAdded Product rendering examples and poster generation examples to AI Art & Animation section.\n\n\n\n6.3 September 5, 2024\n\nAdded GA4(Google Analytics 4).\nAdded vid2vid Lab to AI Art & Animation section.\n\n\n\n6.4 September 4, 2024\n\nUpdated about me.\nUpdated AI Art & Animation section.\n\n\n\n6.5 September 3, 2024\n\nAdded AI Art & Animation section.\nAdded Lab01~10 to ArcGIS section.\nUpdated sidebar in ArcGIS section.\n\n\n\n6.6 September 2, 2024\n\nAdded a new sidebar to the website.\nImproved the color scheme for better readability.\nUpdated the footer with new social media links.\n\n\n\n6.7 September 1, 2024\n\nUpdated CV and about me.\nAdded logo in nav bar.\n\n\n\n6.8 August 30, 2024\n\nAdded back to top function.\nIntegrated GitHub and LinkedIn icons in the navigation bar.\nAdded dark mode.\n\n\n\n6.9 August 25, 2024\n\nAdded Updated Log.\nUpdated GitHub SSH URL repo.\nAdded section for ArcGIS labs."
  },
  {
    "objectID": "img2vid.html",
    "href": "img2vid.html",
    "title": "3. Mix Image to Video Creation",
    "section": "",
    "text": "In this workflow, we combine three different images: a girl, rain, and blue flame, filter the content through a Mask, and use IPAdapter combined with AnimateDiff to create an animation of a dragon girl casting a spell in the rain. This workflow demonstrates how changing the motion scale affects the movement of different elements in the scene.\nFor example, at a lower motion scale of 0.8, the rain and fire elements remain static, while at a higher scale of 1.4, the background becomes distorted, intensifying the animation.\nThe complete workflow file can be found below for reference, and it can be imported into ComfyUI for further adjustments.\nDownload the workflow JSON file\n\nCasting Spell Animation Output1:\n\n\n\n\nExample 1\n\n\n\nCasting Spell Animation Output2:\n\n\n\n\nExample 2",
    "crumbs": [
      "AI Art & Animation",
      "3. Mix Image to Video Creation"
    ]
  },
  {
    "objectID": "img2vid.html#introduction",
    "href": "img2vid.html#introduction",
    "title": "3. Mix Image to Video Creation",
    "section": "",
    "text": "In this workflow, we combine three different images: a girl, rain, and blue flame, filter the content through a Mask, and use IPAdapter combined with AnimateDiff to create an animation of a dragon girl casting a spell in the rain. This workflow demonstrates how changing the motion scale affects the movement of different elements in the scene.\nFor example, at a lower motion scale of 0.8, the rain and fire elements remain static, while at a higher scale of 1.4, the background becomes distorted, intensifying the animation.\nThe complete workflow file can be found below for reference, and it can be imported into ComfyUI for further adjustments.\nDownload the workflow JSON file\n\nCasting Spell Animation Output1:\n\n\n\n\nExample 1\n\n\n\nCasting Spell Animation Output2:\n\n\n\n\nExample 2",
    "crumbs": [
      "AI Art & Animation",
      "3. Mix Image to Video Creation"
    ]
  },
  {
    "objectID": "img2vid.html#motion-scale-examples",
    "href": "img2vid.html#motion-scale-examples",
    "title": "3. Mix Image to Video Creation",
    "section": "2 Motion Scale Examples",
    "text": "2 Motion Scale Examples\nThe following examples show how different motion scales impact the animation:\n\nMotion Scale 0.8: The rain and fire remain static. This scale is suitable for less dynamic movement scenes.\n\n\nVideo\nMotion Scale 0.8\n\n\nMotion Scale 1.4: This introduces background distortion, adding a more dynamic and intense movement.\n\n\nVideo\nMotion Scale 1.4",
    "crumbs": [
      "AI Art & Animation",
      "3. Mix Image to Video Creation"
    ]
  },
  {
    "objectID": "img2vid.html#workflow-overview",
    "href": "img2vid.html#workflow-overview",
    "title": "3. Mix Image to Video Creation",
    "section": "3 Workflow Overview",
    "text": "3 Workflow Overview\n\nWorkflow Setup: \n\n\n3.1 Step 1: Image Selection\nStart by providing both positive and negative keywords that describe the video you wish to create. Select the three primary images: the girl, rain, and blue flame, that will be used in the animation.\n\nThree primary images: \n\n\n\n3.2 Step 2: Create Mask\nApply a Mask to filter the necessary elements of each image for precise animation control. In this workflow, the mask is used to isolate and control different parts of the image, such as the background and the character, to allow for distinct animation effects.\n\nCreating the Mask: After generating the initial image, the mask is created by removing or painting over the areas where the character is present. This involves erasing or blacking out the parts of the image where the character appears. By doing so, the remaining elements (such as the rain and fire) can be controlled separately in the animation, while the masked-out character remains unaffected. This mask ensures that only the intended elements are animated, adding precision to the final video.\nMask Example: \n\nThis approach provides greater flexibility in fine-tuning the animation, allowing you to focus on specific elements while maintaining control over the overall scene.\n\n\n3.3 Step 3: IPAdapter and AnimateDiff Configuration\nIn the IPAdapter here, you can change the weight and weight type of each image, and set the time points when they appear in the animation. For example, in this workflow, the weight type of the images is set to ease in, which allows for a smooth transition at the beginning of the animation.\nAt the same time, the appearance time of the flame is set between 0.3 and 0.9 of the overall animation duration, which makes the flame appear in the middle of the sequence, creating the effect of casting a fire spell. This configuration helps to synchronize the animation’s visual elements for a more dynamic and immersive result.\n\n\n3.4 Step 4: Motion Scale and Batch Size Configuration\nAdjusting the motion scale and batch size parameters allows fine-tuning of the animation. The motion scale directly impacts how much movement or distortion is applied to the elements in the scene, while the batch size controls the number of frames rendered simultaneously.\n\nMotion Scale: As the motion scale increases, the intensity of the animation also increases. For example, at a lower motion scale (e.g., 0.8), elements like rain and fire remain mostly static, adding subtle animation to the scene. As the motion scale increases (e.g., 1.4), these elements begin to move more dynamically, with greater intensity, and the background may become distorted, creating a more dramatic effect. This allows you to control the level of action or motion depending on your desired output.\nBatch Size: The batch size parameter determines how many frames are rendered in each pass. In this workflow, it also dictates the total number of frames in the animation. A larger batch size results in more frames being generated at once, which can be helpful for smoother animation but may increase memory consumption. Conversely, a smaller batch size can reduce memory load but might produce a more segmented animation. In this example, a batch size of 16 was used to balance performance and smoothness.\nMotion Scale & Batch Size Example: \n\nBy adjusting these parameters, you can fine-tune the movement and animation strength while managing performance and rendering time.\n\n\n3.5 Step 5: Upscaling for Enhanced Quality\nUse the Upscale process to improve the resolution and visual quality of the final animation.\n\nBefore and After Upscale Comparison:",
    "crumbs": [
      "AI Art & Animation",
      "3. Mix Image to Video Creation"
    ]
  },
  {
    "objectID": "img2vid.html#conclusion",
    "href": "img2vid.html#conclusion",
    "title": "3. Mix Image to Video Creation",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis workflow showcases the flexibility and power of combining IPAdapter with AnimateDiff to create dynamic animations by transitioning between multiple images with various motion scales and batch sizes. By fine-tuning parameters such as motion scale, batch size, and using masks, you can achieve precise control over different elements in the animation, such as the movement of rain and fire.\nThe results demonstrate that subtle changes in the motion scale can significantly impact the animation intensity, from static background elements to more dynamic and dramatic effects. This workflow is ideal for creating captivating visuals, especially when integrating multiple visual components like characters and environmental effects.\nExperimenting with different configurations can help optimize both the visual output and performance, allowing you to create animations that align perfectly with your creative vision.",
    "crumbs": [
      "AI Art & Animation",
      "3. Mix Image to Video Creation"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "If you would like to learn more about my professional experience and skills, feel free to download my CV below:\nDownload My CV"
  },
  {
    "objectID": "about.html#download-my-cv",
    "href": "about.html#download-my-cv",
    "title": "About Me",
    "section": "",
    "text": "If you would like to learn more about my professional experience and skills, feel free to download my CV below:\nDownload My CV"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "2 About Me",
    "text": "2 About Me\nHi, I’m Chun-Yen Pan, a passionate Data Analyst with expertise in Social Data Analytics and Research. My academic background in Political Science and Big Data Analysis has equipped me with strong data analysis skills using tools like R, Python, and SQL. My focus is on creating data-driven visualizations that enhance research impact.\n\n2.1 Profile Picture\n\n\n\nProfile Picture"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "3 Skills",
    "text": "3 Skills\n\nData Visualization: Creating compelling visual narratives using R, Python, and specialized tools like Tableau, Power BI, and ggplot2.\nMachine Learning: Practical experience with PyTorch, TensorFlow, and other frameworks to build models.\nGeospatial Analysis: Proficient in ArcGIS for spatial data analysis and visualization.\nComfyUI: Skilled in creating and optimizing AI art and animation workflows using ComfyUI.\nProgram Evaluation: Expertise in improving program effectiveness through detailed assessments, utilizing stakeholder feedback, setting up experimental group and control group, and evaluating program outcomes with a focus on evidence-based decisions.\nInternational Law & Political Science: Strong grasp of the relationship between political science and international law, with a focus on regional conflicts and the role of international organizations in global governance."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "4 Education",
    "text": "4 Education\n\nMaster of Science (MS) in Social Data Analytics and Research, The University of Texas at Dallas\nBachelor’s Degree in Political Science with a minor in Big Data Analysis, Soochow University, Taiwan"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "5 Projects",
    "text": "5 Projects\nExplore my GitHub to see detailed project examples.\n\nData Visualization Dashboard\nCreated a Python-based dashboard using Plotly and Dash to analyze financial data, incorporating geopolitical events for insights.\nNetwork Structure of the Digital Advertising Marketplace\nAnalyzed online ad networks using Graph Databases, highlighting connections in digital platform regulations.\nInterstate Affinity Prediction\nDeveloped a machine learning model to predict U.S.-related international relationships using TensorFlow, Keras, and Random Forests.\nSQL Analysis of Chronic Disease Prescriptions\nConducted SQL-based research on prescription patterns across Taiwanese hospitals, revealing trends in chronic disease treatments."
  },
  {
    "objectID": "about.html#languages",
    "href": "about.html#languages",
    "title": "About Me",
    "section": "6 Languages",
    "text": "6 Languages\n\nMandarin: Native proficiency\n\nEnglish: Full professional proficiency\n\nTaiwanese: Conversational proficiency"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About Me",
    "section": "7 Interests",
    "text": "7 Interests\nOutside of work, I enjoy swimming, hiking, reading the news, and playing Japanese Mahjong and other board games."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "8 Contact",
    "text": "8 Contact\n\nEmail: jimpan0612@gmail.com\nGitHub: Jimpan0612\nLinkedIn: Chun-Yen Pan\nPersonal Website: MyWebsite"
  },
  {
    "objectID": "6381Lab02.html",
    "href": "6381Lab02.html",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "",
    "text": "In this lab, the goal was to learn about basic methods for map projections in ArcGIS Pro. The task involved creating maps of Minnesota in three different statewide projections, as well as a map of reprojected Minnesota county boundaries with an inset global view. Additionally, the lab involved recording areas and coordinates for various projections and measurements.",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab02.html#introduction",
    "href": "6381Lab02.html#introduction",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "",
    "text": "In this lab, the goal was to learn about basic methods for map projections in ArcGIS Pro. The task involved creating maps of Minnesota in three different statewide projections, as well as a map of reprojected Minnesota county boundaries with an inset global view. Additionally, the lab involved recording areas and coordinates for various projections and measurements.",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab02.html#objectives",
    "href": "6381Lab02.html#objectives",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nUnderstand the differences in map projections and how they affect spatial data.\nCreate maps using different projections: Albers, UTM, and Mercator.\nLearn how to use on-the-fly projections in ArcGIS Pro.\nMeasure distances and coordinates in different projections.",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab02.html#tasks",
    "href": "6381Lab02.html#tasks",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Create and Compare Maps\nThree maps were created to compare different projections:\n\nAlbers Projection\nUTM Projection\nMercator Projection\n\nThese maps were then analyzed to understand how the projections affect the shape and measurements within the state of Minnesota.\n\n\n3.2 2. Measure Distances\nUsing the measure tool in ArcGIS Pro, distances between specific points were measured in each of the projections. For example, the distance from the northeastern-most point of Minnesota to the southwestern-most point was calculated in kilometers for all three projections.\n\n\n3.3 3. Record Coordinates\nCoordinates for the northeast corner of Ramsey County were recorded in three projections: Albers, UTM, and Custom Mercator. This step highlighted how different projections can result in different coordinate values for the same geographic location.",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab02.html#results",
    "href": "6381Lab02.html#results",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Distance Across Minnesota\n\nAlbers Projection: 738.38 km\nUTM Zone 15N: 739.11 km\nCustom Mercator: 1059.32 km\n\n\n\n4.2 Coordinates of Northeast Corner of Ramsey County\n\n\n\nProjection\nX-Coordinate\nY-Coordinate\n\n\n\n\nAlbers (Meters)\n237015.86E\n2463303.93N\n\n\nUTM Zone 15 (Meters)\n501167.40E\n4996740.42N\n\n\nCustom Mercator\n10351044.86W\n5610717.55N\n\n\n\n\n\n4.3 Final Maps\n\n4.3.1 Comparison of Three Map Projections\n\n\n\nCompare of Three Map Projections\n\n\n\n\n4.3.2 Minnesota Counties - UTM Projection\n\n\n\nMinnesota Counties - UTM Projection",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab02.html#conclusion",
    "href": "6381Lab02.html#conclusion",
    "title": "Lab 02: Projections in ArcGIS",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis lab provided valuable insights into the importance of map projections in GIS. By working with different projections, it became clear how much projections can impact spatial data, particularly in terms of shape and distance measurements. Understanding and correctly applying map projections is crucial for any spatial analysis in GIS.",
    "crumbs": [
      "ArcGIS",
      "Lab 02: Projections in ArcGIS"
    ]
  },
  {
    "objectID": "6381Lab01.html",
    "href": "6381Lab01.html",
    "title": "Lab 01: Introduction to ArcGIS and Basic Map Operations",
    "section": "",
    "text": "This lab introduces ArcGIS software and covers basic map operations, including working with shapefiles, creating new layers, and exporting maps.",
    "crumbs": [
      "ArcGIS",
      "Lab 01: Introduction to ArcGIS and Basic Map Operations"
    ]
  },
  {
    "objectID": "6381Lab01.html#objective",
    "href": "6381Lab01.html#objective",
    "title": "Lab 01: Introduction to ArcGIS and Basic Map Operations",
    "section": "",
    "text": "This lab introduces ArcGIS software and covers basic map operations, including working with shapefiles, creating new layers, and exporting maps.",
    "crumbs": [
      "ArcGIS",
      "Lab 01: Introduction to ArcGIS and Basic Map Operations"
    ]
  },
  {
    "objectID": "6381Lab01.html#procedure",
    "href": "6381Lab01.html#procedure",
    "title": "Lab 01: Introduction to ArcGIS and Basic Map Operations",
    "section": "2 Procedure",
    "text": "2 Procedure\n\nIntroduction to ArcGIS: Familiarized with the ArcGIS interface and explored essential tools and functions.\nWorking with Shapefiles: Imported and managed shapefiles, with a focus on understanding attribute tables and layer properties.\nCreating New Layers: Developed new layers by extracting specific features and attributes.\nExporting Maps: Exported maps in PDF format for documentation and presentation purposes.",
    "crumbs": [
      "ArcGIS",
      "Lab 01: Introduction to ArcGIS and Basic Map Operations"
    ]
  },
  {
    "objectID": "6381Lab01.html#results",
    "href": "6381Lab01.html#results",
    "title": "Lab 01: Introduction to ArcGIS and Basic Map Operations",
    "section": "3 Results",
    "text": "3 Results\nBelow are the results generated during the lab:\n\n3.1 Cloquet Polygon Map\n\n\n\nCloquet Polygon Map\n\n\n\n\n3.2 Cloquet Map\n\n\n\nCloquet Map\n\n\n\n\n3.3 Hugomap\n\n\n\nHugomap\n\n\n\n\n3.4 Wetlands Map\n\n\n\nWetlands Map",
    "crumbs": [
      "ArcGIS",
      "Lab 01: Introduction to ArcGIS and Basic Map Operations"
    ]
  },
  {
    "objectID": "6381Lab01.html#conclusion",
    "href": "6381Lab01.html#conclusion",
    "title": "Lab 01: Introduction to ArcGIS and Basic Map Operations",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis lab provided an essential introduction to ArcGIS, offering practical experience in handling shapefiles and generating exportable map products. The skills developed in this lab are foundational for more advanced GIS operations.",
    "crumbs": [
      "ArcGIS",
      "Lab 01: Introduction to ArcGIS and Basic Map Operations"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html",
    "href": "6323Lab_caret01.html",
    "title": "Lab: caret Random Forest",
    "section": "",
    "text": "In this lab, we will explore the caret package and its use for building Random Forest models. This includes both classification and regression problems, using the iris and mtcars datasets as examples.\nWe reference the works of Kuhn et al. (2008) and the caret documentation (2020) for additional insight into building predictive models.\n\n\n\n# install.packages(c(\"caret\", \"dplyr\", \"ggplot2\", \"tidyr\"))\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#load-required-libraries",
    "href": "6323Lab_caret01.html#load-required-libraries",
    "title": "Lab: caret Random Forest",
    "section": "",
    "text": "# install.packages(c(\"caret\", \"dplyr\", \"ggplot2\", \"tidyr\"))\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#data-preparation",
    "href": "6323Lab_caret01.html#data-preparation",
    "title": "Lab: caret Random Forest",
    "section": "2.1 Data Preparation",
    "text": "2.1 Data Preparation\nFirst, load the iris dataset and visualize the relationship between Sepal length, width, and species.\n\n# Load the iris dataset\ndata(iris)\n\n# Examine the dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Data visualization\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  theme_bw()",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#traintest-split",
    "href": "6323Lab_caret01.html#traintest-split",
    "title": "Lab: caret Random Forest",
    "section": "2.2 Train/Test Split",
    "text": "2.2 Train/Test Split\nNext, split the data into a training set (70%) and a testing set (30%).\n\n# Split the data into training and testing sets (70% train, 30% test)\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(iris), 0.7 * nrow(iris))\ntrain_data &lt;- iris[train_index, ]\ntest_data &lt;- iris[-train_index, ]",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#train-the-random-forest-model",
    "href": "6323Lab_caret01.html#train-the-random-forest-model",
    "title": "Lab: caret Random Forest",
    "section": "2.3 Train the Random Forest Model",
    "text": "2.3 Train the Random Forest Model\nWe set up 10-fold cross-validation and train the model using the train() function from the caret package.\n\n# Set up the training control\ntrain_control &lt;- trainControl(method = \"cv\", number = 10) # 10-fold Cross-Validation\n\n# Train the model\nset.seed(123)\nmodel &lt;- caret::train(Species ~ ., data = train_data,\n               method = \"rf\", # Random Forest\n               trControl = train_control,\n               tuneLength = 3,\n               preProcess = c(\"center\", \"scale\"))\n\n# Print the model details\nprint(model)\n\nRandom Forest \n\n105 samples\n  4 predictor\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nPre-processing: centered (4), scaled (4) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 95, 95, 95, 95, 93, 95, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.9518182  0.9274934\n  3     0.9518182  0.9274934\n  4     0.9518182  0.9274934\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#evaluate-the-model",
    "href": "6323Lab_caret01.html#evaluate-the-model",
    "title": "Lab: caret Random Forest",
    "section": "2.4 Evaluate the Model",
    "text": "2.4 Evaluate the Model\nFinally, predict the species for the test data and calculate the accuracy of the model.\n\n# Make predictions on the test data\npredictions &lt;- predict(model, test_data)\n\n# Calculate the accuracy of the model\naccuracy &lt;- mean(predictions == test_data$Species)\ncat(\"Accuracy:\", accuracy)\n\nAccuracy: 0.9777778",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#data-preparation-1",
    "href": "6323Lab_caret01.html#data-preparation-1",
    "title": "Lab: caret Random Forest",
    "section": "3.1 Data Preparation",
    "text": "3.1 Data Preparation\nLoad the dataset and split it similarly to the previous example.\n\n# Load the mtcars dataset\ndata(mtcars)\n\n# Examine the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n# Split the data into training and testing sets (70% train, 30% test)\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(mtcars), 0.7 * nrow(mtcars))\ntrain_data &lt;- mtcars[train_index, ]\ntest_data &lt;- mtcars[-train_index, ]",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#train-the-random-forest-model-1",
    "href": "6323Lab_caret01.html#train-the-random-forest-model-1",
    "title": "Lab: caret Random Forest",
    "section": "3.2 Train the Random Forest Model",
    "text": "3.2 Train the Random Forest Model\nWe now train a random forest regression model to predict the mpg variable.\n\n# Set up the training control\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the regression model\nset.seed(123)\nmodel &lt;- train(mpg ~ ., data = train_data,\n               method = \"rf\",\n               trControl = train_control,\n               tuneLength = 3,\n               preProcess = c(\"center\", \"scale\"))\n\n# Print the model details\nprint(model)\n\nRandom Forest \n\n22 samples\n10 predictors\n\nPre-processing: centered (10), scaled (10) \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 19, 20, 20, 20, 20, 20, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared   MAE     \n   2    2.796874  0.9759378  2.437864\n   6    2.636784  0.9765995  2.276657\n  10    2.627549  0.9745273  2.270757\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 10.",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6323Lab_caret01.html#evaluate-the-model-1",
    "href": "6323Lab_caret01.html#evaluate-the-model-1",
    "title": "Lab: caret Random Forest",
    "section": "3.3 Evaluate the Model",
    "text": "3.3 Evaluate the Model\nEvaluate the model’s performance using the Root Mean Squared Error (RMSE) on the test data.\n\n# Make predictions on the test data\npredictions &lt;- predict(model, test_data)\n\n# Calculate the RMSE (Root Mean Squared Error) of the model\nRMSE &lt;- sqrt(mean((predictions - test_data$mpg)^2))\ncat(\"RMSE:\", RMSE)\n\nRMSE: 2.003634",
    "crumbs": [
      "Lab: caret Random Forest"
    ]
  },
  {
    "objectID": "6381Lab04.html",
    "href": "6381Lab04.html",
    "title": "Lab 04: Digitizing and Topology",
    "section": "",
    "text": "This lab focuses on digitizing and creating topological rules within ArcGIS. The main tasks involve setting up a digitizing environment, creating and editing feature layers, and applying topological rules to ensure the accuracy and integrity of the digitized data.",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "6381Lab04.html#introduction",
    "href": "6381Lab04.html#introduction",
    "title": "Lab 04: Digitizing and Topology",
    "section": "",
    "text": "This lab focuses on digitizing and creating topological rules within ArcGIS. The main tasks involve setting up a digitizing environment, creating and editing feature layers, and applying topological rules to ensure the accuracy and integrity of the digitized data.",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "6381Lab04.html#objectives",
    "href": "6381Lab04.html#objectives",
    "title": "Lab 04: Digitizing and Topology",
    "section": "2 Objectives",
    "text": "2 Objectives\n\nUnderstand the process of digitizing features using ArcGIS Pro.\nApply topological rules to maintain spatial relationships between digitized features.\nValidate and correct topological errors within the dataset.",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "6381Lab04.html#tasks",
    "href": "6381Lab04.html#tasks",
    "title": "Lab 04: Digitizing and Topology",
    "section": "3 Tasks",
    "text": "3 Tasks\n\n3.1 1. Preparing the Data\n\nStart ArcGIS Pro and create a new map project.\nLoad the provided RectSpring image and add the Lab4AP.gdb to the project.\nInclude the necessary layers (NWI, MNDOT, DNR lakes, and SouthBayArea feature classes).\n\n\n\n3.2 2. Digitizing Upland and Lake Boundaries\n\nUse the RectSpring image to digitize the upland/lake boundary within the SouthBayArea.\nDigitize aquatic vegetation using both the RectSpring and BigMarSum images.\nEnsure the boundaries are accurate by switching between images for better visual cues.\n\n\n\n3.3 3. Creating and Validating Topology\n\nSet up a topology for the digitized layers within the feature dataset.\nDefine the necessary topology rules to prevent overlaps, gaps, and ensure spatial integrity.\nValidate the topology and identify any errors.",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "6381Lab04.html#results",
    "href": "6381Lab04.html#results",
    "title": "Lab 04: Digitizing and Topology",
    "section": "4 Results",
    "text": "4 Results\n\n4.1 Digitized and Validated Topology\nThe following images show the results of the digitizing and topology validation process.\n\n4.1.1 Uplands, Lakes, Aquatic Vegetation, Big Lake by Jim\n\n\n\nUplands, Lakes, Aquatic Vegetation, Big Lake",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "6381Lab04.html#conclusion",
    "href": "6381Lab04.html#conclusion",
    "title": "Lab 04: Digitizing and Topology",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nDigitizing and topology validation are crucial processes in GIS to ensure that spatial data is accurate and reliable. By following proper digitizing techniques and applying topological rules, one can maintain the integrity of spatial relationships in the dataset. This lab provided hands-on experience in setting.",
    "crumbs": [
      "ArcGIS",
      "Lab 04: Digitizing and Topology"
    ]
  },
  {
    "objectID": "2imgbatch_img2vid.html",
    "href": "2imgbatch_img2vid.html",
    "title": "2. Two Image Batch to Video Creation",
    "section": "",
    "text": "In this workflow, we use the IPAdapter to create an animation by transitioning between two similarly styled images. This method is effective for animations like eye blinking, where subtle differences (such as open and closed eyes) are applied between frames.\nThe process involves configuring AnimateDiff and using upscaling to achieve higher-quality results while optimizing memory usage. The complete workflow can be found below, and you can import the JSON file into ComfyUI by dragging it into the interface.\nDownload the workflow JSON file\n\nEyes Blink Animation Output:\n\n\n\n\nEyes Blink Animation",
    "crumbs": [
      "AI Art & Animation",
      "2. Two Image Batch to Video Creation"
    ]
  },
  {
    "objectID": "2imgbatch_img2vid.html#introduction",
    "href": "2imgbatch_img2vid.html#introduction",
    "title": "2. Two Image Batch to Video Creation",
    "section": "",
    "text": "In this workflow, we use the IPAdapter to create an animation by transitioning between two similarly styled images. This method is effective for animations like eye blinking, where subtle differences (such as open and closed eyes) are applied between frames.\nThe process involves configuring AnimateDiff and using upscaling to achieve higher-quality results while optimizing memory usage. The complete workflow can be found below, and you can import the JSON file into ComfyUI by dragging it into the interface.\nDownload the workflow JSON file\n\nEyes Blink Animation Output:\n\n\n\n\nEyes Blink Animation",
    "crumbs": [
      "AI Art & Animation",
      "2. Two Image Batch to Video Creation"
    ]
  },
  {
    "objectID": "2imgbatch_img2vid.html#workflow-overview",
    "href": "2imgbatch_img2vid.html#workflow-overview",
    "title": "2. Two Image Batch to Video Creation",
    "section": "2 Workflow Overview",
    "text": "2 Workflow Overview\n\nWorkflow Setup: \n\n\n2.1 Step 1: Image Selection\nStart by providing both positive and negative keywords that describe the video you wish to create. Select two images of the same character with subtle differences. For example, to create a blinking animation, use one image where the character’s eyes are open and another where the eyes are closed.\n\n2 batch images: \nClosed Eyes image : \n\n\n\n2.2 Step 2: IPAdapter and AnimateDiff Configuration\nThe next step is setting up IPAdapter and AnimateDiff. You load both images into the animation workflow and configure the frame rate, loop count, and time distribution between the two images. Adjust the animation length by giving each image a weight based on how long it should be displayed in the animation.\n\n\n2.3 Step 3: Generate Initial Animation\nRun the animation with AnimateDiff and KSampler to produce a transition between the two images. In this case, Image 1 (open eyes) will show for 50% of the duration, and Image 2 (closed eyes) will appear for 50%.\n\n\n2.4 Step 4: Upscaling for Enhanced Quality\nAfter generating the animation, we upscale the latent space using the Upscale Latent By node. This increases the resolution and enhances the details, allowing us to keep the memory usage low while maintaining quality.\n\nUpscaling Process: \n\n\n\n2.5 Step 5: Comparing Results\nThe upscaling process significantly improves the image quality, reducing blurriness and enhancing finer details. Here’s a comparison between the original and upscaled images:\n\nBefore and After Upscale Comparison:",
    "crumbs": [
      "AI Art & Animation",
      "2. Two Image Batch to Video Creation"
    ]
  },
  {
    "objectID": "2imgbatch_img2vid.html#conclusion",
    "href": "2imgbatch_img2vid.html#conclusion",
    "title": "2. Two Image Batch to Video Creation",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nBy using the IPAdapter and AnimateDiff workflow, you can smoothly animate transitions between two images. Upscaling the latent space allows for high-quality animations without heavy resource consumption. This method is efficient for creating AI-generated animations like blinking or other subtle movements, as seen in the final output here.",
    "crumbs": [
      "AI Art & Animation",
      "2. Two Image Batch to Video Creation"
    ]
  },
  {
    "objectID": "6302Lab03.html",
    "href": "6302Lab03.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "1 R Programming (EDA)\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\n2 Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "6302Lab01.html",
    "href": "6302Lab01.html",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=TRUE) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.997615\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"blue\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"blue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#create-object-using-the-assignment-operator--",
    "href": "6302Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#using-function",
    "href": "6302Lab01.html#using-function",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#using---operators",
    "href": "6302Lab01.html#using---operators",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#matrix-operations",
    "href": "6302Lab01.html#matrix-operations",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=TRUE) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.997615\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#simple-descriptive-statistics",
    "href": "6302Lab01.html#simple-descriptive-statistics",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  },
  {
    "objectID": "6302Lab01.html#graphics-using-r-graphics-without-packages",
    "href": "6302Lab01.html#graphics-using-r-graphics-without-packages",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"blue\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"blue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x",
    "crumbs": [
      "EPPS 6302: Lab01"
    ]
  }
]